---
title: "A Deeper Dive Into COVID-Related Relationships"
author: "Tracy Huang, Helen Feibes, Henry Bassett"
date: "12/15/21"
output:
  rmdformats::readthedown:
    thumbnails: false
    highlight: "kate"
    self_contained: false
---

```{r setup, include = FALSE}
library(tidyverse)
library(knitr)

# Set code chunk defaults 
knitr::opts_chunk$set(echo = FALSE, 
                      mesage = FALSE,
                      warning = FALSE,
                      fig.align = "center")

# Set R environment options
options(knitr.kable.NA = '')
```

# Introduction
Given the devastating effects of the      , we were interested in exploring different relationships 

# COVID State Restrictions Network

We wanted to explore how different states are connected by how many days in 2020 they shared the same COVID restrictions (restaurant restrictions, bar restrictions, mask mandates, gathering bans, and stay at home orders). Hence, we were interested in creating a network, with each node representing a state and each edge connecting two states together. The nodes are colored by the proportion of annual COVID cases per state population, and the edges are weighted by how many days each pair of states shared the same restrictions in 2020. Users can also select the exact states to visualize (we recommend looking at five states or less; otherwise the network becomes too crowded!), as well as the type of COVID restriction to explore. There is a drop-down menu for states above the network; when a state is selected, its edges with other states will be highlighted so they can be more easily seen. Lastly, users can hover over the edges in the network to see the exact number of days that each pair of states shared.

```{r, out.extra="data-external=1"}
knitr::include_app("https://th23.shinyapps.io/blog_THeHe/", 
                   height = "1000px")
```
Visit [our Shiny app](https://th23.shinyapps.io/blog_THeHe/) to explore and interact with the COVID state restrictions network!

## Data Sources 

### Data-sets 

For the COVID state restrictions network, we used data from the Centers for Disease Control and Prevention (CDC) on state-issued prevention measures for COVID-19. The specific data-sets we used were:

* `Restaurants.csv`, which contains U.S. state and territorial orders closing and reopening restaurants issued from March 15, 2020 through August 15, 2021 by county by day

* `Bars.csv`, which contains U.S. state and territorial orders closing and reopening bars issued from March 15, 2020 through August 15, 2021 by county by day

* `Mask_Mandates.csv`, which contains U.S. state and territorial public mask mandates from April 10, 2020 through August 15, 2021 by county by day

* `Gathering_Bans.csv`, which contains U.S. state and territorial gathering bans from March 11, 2020 through August 15, 2021 by county by day

* `Stay-At-Home_Orders.csv`, which contains U.S. state and territorial public mask mandates from March 15, 2020 through August 15, 2021 by county by day

Visit [the CDC website](https://data.cdc.gov/browse?category=Policy+Surveillance) to explore and download the data-sets. 

### Data Wrangling

_The following code chunks were for wrangling the `Restaurants.csv` data set, but the exact same procedure was repeated with the other four data sets._ 

In order to build a network diagram using the `igraph` package in R, we needed to transform the raw data into an edge list, a data frame where there are three columns; the first column would contain the origin states, the second column would contain the target states, and the third column would contain the number of days the states in the first and second column share. After getting all five raw data sets into this format, we converted them into an `igraph` object using the `graph_from_data_frame` function in the `igraph` package. We also specified that it was an un-directed network by putting FALSE in the `directed` argument, since it did not matter which direction the edges went between each pair of states.

We then wanted to use the `visNetwork` package to create interactive network visualizations. We converted the `igraph` objects into a format usable for `visNetwork` using the `toVisNetworkData` function. 

```{r echo=TRUE, eval=FALSE}
# Create an igraph object
restaurants_igraph <- graph_from_data_frame(restaurants_final, directed = FALSE)

# Convert igraph to visNetwork format
restaurants_visNetwork <- toVisNetworkData(restaurants_igraph)
```

The `visNetwork` package requires the edges in the network to be a data frame. The original edges data frame in the `visNetwork` object contained three columns: a "from" column and a "to" column that taken together, contained all possible combinations of two states, as well as a column for the number of days each pair of states shared. However, `visNetwork` allows us to add properties to edges by simply adding columns to the data frame. We thus added a column for edge width and edge labels using the `mutate` function from `dplyr`.

```{r echo=TRUE, eval=FALSE}
# Wrangle edges data
restaurants_edges <- restaurants_visNetwork$edges %>%
  # Create a new column for edge width
  mutate(value = as.numeric(restaurants_overLap),
         width = value/100) %>%
  select(-value) 

restaurants_edges <- restaurants_edges %>%
  # Create a new column for edge labels that appear when hovered over
  mutate(title = paste0(restaurants_edges$restaurants_overLap, " days shared in 2020"))
```

Similarly, the `visNetwork` package requires the nodes in the network to be a data frame. The original nodes data frame in the `visNetwork` object originally contained two columns, an "id" column and a "label" column that contained the state names for the nodes. However, we also added a new column for the color of each node, where color corresponded to the proportion of annual COVID cases per state population. The `eigScalePal` function creates a gradient of five colors from the `colorRampPalette` function, and the `mutate` command breaks the COVID cases proportions column values into five sections, assigning each value to one out of the five colors.

```{r echo=TRUE, eval=FALSE}
# Set color palette
eigScalePal <- colorRampPalette(c("blue", "red"), bias = 5)
num_colors <- 5

# Wrangle nodes data
restaurants_nodes <- restaurants_visNetwork$nodes %>%
  # Join annual cases dataset
  inner_join(annual_cases, by = c("id" = "state")) %>%
  # Create a new column for node color, colored by annual COVID cases in relation to color palette legend
  mutate(color = eigScalePal(num_colors)[cut(covid_prop, breaks = num_colors)]) %>%
  select(-c(covid_prop))
```

## Creating the interactive network using `visNetwork` in Shiny  

In order to create an interactive network in a Shiny app, the function, `visNetwork`, can be used within the `renderVisNetwork` function in the Shiny server code. The `visNetwork` function needs two arguments: the nodes and the edges. Using the pipe function, there are also many other functions that can be used to further customize the network. For example, the `visOptions` function allows users to select a node and have its connecting edges highlighted. Another example is the `visEdges` function that specifies the color of the edges and the different color they should change to when selected. 

To be able to add and remove nodes interactively, depending on what the user selects, an updated network can be created using the `visNetworkProxy` function. After creating data frames to contain the nodes that the user selected and did not select, `visRemoveNodes` and `visUpdateNodes` can be used to reactively display only the nodes the user selected in the network.

```{r echo=TRUE, eval=FALSE}
server <- function(input, output) {
  
  output$network_proxy_update <- renderVisNetwork({
    visNetwork(active_nodes(), active_edges(), height = "700px", width = "100%", 
               main = list(text = "Network for States Sharing the Same COVID Restrictions in 2020", 
                           style = "font-family:Arial;font-size:20px"
               )) %>%
      visNodes(size = 10) %>%
      visOptions(selectedBy = "group", 
                 highlightNearest = list(enabled = TRUE, hover = TRUE), 
                 nodesIdSelection = TRUE) %>%
      visPhysics(stabilization = FALSE) %>%
      visEdges(color = list(color = "black", highlight = "red")) 
  })
  
  myVisNetworkProxy <- visNetworkProxy("network_proxy_update")
  
  observe ({
    # Create a data frame to contain the nodes the user selects
    filteredNodes <- active_nodes()[gathering_ban_nodes$id %in% input$filterNodes, ,drop = FALSE]
    # Create a data frame to contain all the nodes the user did not select
    hiddenNodes <- anti_join(active_nodes(), filteredNodes)
    visRemoveNodes(myVisNetworkProxy, id = hiddenNodes$id)
    visUpdateNodes(myVisNetworkProxy, nodes = filteredNodes)
  })
}
```

## Findings 

Overall, the network allows users to explore the relationships between different states of their choice in terms of how many days in 2020 they shared the same COVID restrictions, as well as specify which COVID restriction they wanted to look at. There are many different analyses that can come from exploring the network, but it's interesting to see that there is not one dominant pattern or takeaway. Some states with radically different annual COVID cases per population share the same state restrictions for the majority of 2020, whereas some states with very similar annual COVID cases per population share very little of the same state restrictions in 2020. This underlies the complexity of COVID severity in states, and it's important to remember that state restrictions are only one factor within many that influence the proportion of COVID cases that emerge in different locations.

## Limitations 

What are the limitations of your work? Be clear so that others do not misinterpret your findings. To what population do your results apply? Do they generalize? Could your work be extended with more data or computational power or time to analyze? How could your study be improved? Suggesting plausible extensions doesn’t weaken your work; it strengthens it by connecting it to future work. 

For example, you can include **Bold** and _Italic_ and `Code` text.  

You should test out updating your GitHub Pages website:

* clone your group's blog project repo in RStudio
* update "Your Project Title Here" to a new title in the YAML header
* knit `index.Rmd` (we will now knit to HTML by default instead of pdf)
* commit and push **both** `index.Rmd` and `index.html`
* go to https://stat231-f21.github.io/blog_THeHe/ to see the published test document (this is publicly available!)

# Word Map 

```{r load-data, echo = FALSE}
headlines_ex <- read_csv("data/news/headlines.csv", n_max = 5)
```

## Introduction

## Data Sources

The data for the text analysis portion of the project was scraped from local news sites from all fifty states. We found the most widely circulated news outlet in each state (according to [this media source](https://www.agilitypr.com/resources/top-media-outlets/)). At first, we attempted to scrape each news site individually, but this proved very challenging because each site was built differently, so this effort would have required fifty different individualized scraping scripts, which was not feasible. We turned to [Google News](https://news.google.com) as our solution; using this site, we can easily search individual news sites and get back results in a standardized manner.

We were chiefly interested in the sentiments and discussions revolving around the COVID-19 pandemic, so we only searched for articles that included the keyword **"covid"**. Additionally, we restricted each search to time frames that matched up with the time frames that we used in our originally analysis, with the Household Pulse Survey data.

We ran into some issues at first, since Google does not allow scraping of Google News' search results page, which would have been a relatively easy way to obtain headlines. We were able to obtain RSS feeds of Google News searches: these are outputs in XML format that are typically used for email feeds and the like. There was no explicit restriction against scraping these feeds, so we used this output along with the `tidyRSS` package to obtain certain parts of the feed---specifically, headlines. (We were also somewhat ethically assuaged by the fact that each individual news website had no issue with scraping their headlines.)

The specifics of scraping involved nested `for` loops: we had to scrape news sites from fifty states (taken care of by the outer loop), and for each state, we had to scrape headlines for each of the eighteen "weeks" that we were interested in (the inner loop).

```{r scraping-code, eval = FALSE}
# Loop for 50 states
for (i in 1:50) {
  
  # Set current state
  state <- news_sites$state[i]
  #Set current news url
  site <- news_sites$url[i]
  
  # Loop for 18 weeks
  for(j in 1:18){
    
    # Set current start date (of time period in question)
    start_date <- weeks$start[j]
    # Set current end date (of time period in question)
    end_date <- weeks$end[j]
    
    # Create Google News RSS url to scrape
    rss_url <- paste0("https://news.google.com/rss/search?q=before%3A", end_date, "%20after%3A", start_date, "%20covid%20", state, "%20site%3A", site, "&hl=en-US&gl=US&ceid=US%3Aen")
    
    # Scrape data from RSS feed
    headlines_scraped <- tryCatch(
      
      # Return NA instead of headline text when error is thrown (usually because there are no articles that meet criteria)
      error = function(cnd) {
        return(NA)
      },
      
      # Otherwise, pull headline text from the RSS feed
      scraped_text <- rss_url %>% 
        tidyfeed() %>% 
        select(item_title)
    )
    
    # Add scraped headline into our pre-made list
    headlines$headline_text[(i-1)*18 + j] <- headlines_scraped
    
  }
}
```

Once the scraping was completed, we had an (enormous) .csv file with an observation for every headline scraped, tagged with the state of the news outlet that it came from and the week that the headlines ran.

```{r example-headlines, echo = FALSE}
head(headlines_ex)
```

## Data Wrangling

Once we obtained the raw text of every COVID-related headline from Week 22 to Week 39, we began performing text analysis on these headlines. The first wrangling was a simple word frequency analysis, breaking the headlines up and then tallying how often each one appeared (still separated by state and by week, so the tallies were specific to a certain location and time period, i.e. how many times the word **vaccine** appeared in headlines from Massachusetts during Week 33).

```{r freq-analysis, eval = FALSE}
word_frequencies <- all_headlines %>% 
  # Break headlines up into unigrams
  unnest_tokens(output = word, input = headline_text, drop = TRUE) %>% 
  # Remove typical stop words
  anti_join(stop_words, by = "word") %>% 
  # Add word frequencies
  add_count(state, week, word) %>% 
  # Remove duplicate entries
  distinct() %>% 
  # Group words by state & week, then sort by frequency
  arrange(state, week, desc(n))
```

We then did some custom filtering of the unigrams that we ended up with: we had already filtered out the typical stop words from the built-in dictionary, but looking through the data, it was clear that there were some more words that would not be useful to any of our analysis. For example, the names of the newspapers frequently showed up in the headlines that we scraped (i.e. "Massachusetts COVID-19 Cases Spiking Once Again -- Boston Globe"), and clearly if they were in every headline, then they would clog up our frequency analysis. We therefore removed any newspaper-related and any location-specific words from the raw text, as well as any "textual detritus, such as urls or numbers:

```{r custom-filtering, eval = FALSE}
words_wrangled <- word_frequencies %>% 
         # Filter out any numbers
  filter(!str_detect(word, "[:digit:]"),
         # Filter out any phrase with "." (mostly urls)
         !str_detect(word, "\\."),
         # Filter out "covid"
         !str_detect(word, "covid"),
         # Filter out the state names
         !str_detect(word, tolower(as.character(word_frequencies$state)))) %>%
  # Remove custom stop words
  anti_join(custom_stops, by = "word")
```

With that, our text was wrangled, and we were ready for our final analysis.

## Conclusions

## Limitations

# Racial/Ethnic Groups: Clustering & Networks

## Introduction

## Data Sources

## Data Wrangling

## Conclusions

## Limitations




## Including links and images/videos

You can include [links](https://www.datadreaming.org/post/r-markdown-theme-gallery/) and there are a few ways to embed  images! Both options for embedding images below can be used interchangeably. They both work for png, pdf, jpg, and even gif formats, and both support filepaths that are either URLs (for videos, you can include links to any valid YouTube or Vimeo URLs; see [here](https://bookdown.org/yihui/rmarkdown/learnr-videos.html) for more details) or point to a location within your project directory. 

### Option 1: Markdown approach

<!-- ![This is a figure caption. The artwork is called Safe Space by  [Kenesha Sneed](https://www.keneshasneed.com/#/safespace/)](img/Kenesha-Sneed_safe-space.jpeg) -->


### Option 2: Code chunk approach

```{r, fig.cap = "This is also figure caption"}
knitr::include_graphics("https://media.giphy.com/media/H7ZrrA9V2pd3Tehdds/giphy.gif")
```

# You can even create tabs within your webpage if you want! {.tabset .tabset-fade .tabset-pills}

Every subsection heading (starting with `##`) until you create a new section heading (starting with `#`) will be a new tab.

# Customizing your blog design

As a *final* detail **only** if you have time, you can explore options for customizing the style of your blog. By default, we are using the `readthedown` theme from the [**rmdformats** package](https://github.com/juba/rmdformats) (see Line 6 of this file if you want to switch out themes). There are, I'm sure, many many many more similar packages with built in themes, or you can look into how to include a CSS code chunk to customize aspects of the current theme.  

There are some easy-to-change options that you can play around with:

* The theme itself (Line 6): `rmdformats::readthedown`, `rmdformats::downcute`, `rmdformats::robobook`, `rmdformats::material`,
  * For `downcute` only, you can add a new indented line below Line 6 with the code `downcute_theme: "chaos"` for the `downcute chaos` theme
  * I would *not* recommend the other themes that do not have the sidebar navigation
  
* Syntax highlighting options (Line 8, `highlight`): `"default"`, `"tango"`, `"pygments"`, `"kate"`, `"monochrome"`, `"espresso"`, `"zenburn"`, `"haddock"`, or `"textmate"` (or `NULL` for no syntax highlighting)


You can explore additional customizable YAML options by looking at the [**rmdformats** package](https://github.com/juba/rmdformats) page or running, for example, `?rmdformats::readthedown()` to see the help documentation for a particular theme from the package.
